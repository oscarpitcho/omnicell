{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 08:46:45,079 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/DatlingerBock2017.yaml\n",
      "2025-02-20 08:46:45,081 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/SchraivogelSteinmetz2020_TAP_SCREEN__chromosome_11_screen.yaml\n",
      "2025-02-20 08:46:45,083 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/ReplogleWeissman2022_K562_essential.yaml\n",
      "2025-02-20 08:46:45,084 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/GasperiniShendure2019_atscale.yaml\n",
      "2025-02-20 08:46:45,086 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/TianKampmann2021_CRISPRa.yaml\n",
      "2025-02-20 08:46:45,087 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/repogle_k562_essential_raw.yaml\n",
      "2025-02-20 08:46:45,088 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/FrangiehIzar2021_RNA.yaml\n",
      "2025-02-20 08:46:45,090 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/AdamsonWeissman2016_GSM2406681_10X010.yaml\n",
      "2025-02-20 08:46:45,091 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/PapalexiSatija2021_eccite_RNA.yaml\n",
      "2025-02-20 08:46:45,092 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/TianKampmann2021_CRISPRi.yaml\n",
      "2025-02-20 08:46:45,093 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/ChangYe2021.yaml\n",
      "2025-02-20 08:46:45,095 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/TianKampmann2019_iPSC.yaml\n",
      "2025-02-20 08:46:45,096 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/PapalexiSatija2021_eccite_arrayed_RNA.yaml\n",
      "2025-02-20 08:46:45,097 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/AdamsonWeissman2016_GSM2406677_10X005.yaml\n",
      "2025-02-20 08:46:45,098 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/SrivatsanTrapnell2020_sciplex4.yaml\n",
      "2025-02-20 08:46:45,099 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/GehringPachter2019.yaml\n",
      "2025-02-20 08:46:45,101 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/ShifrutMarson2018.yaml\n",
      "2025-02-20 08:46:45,102 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/WeinrebKlein2020.yaml\n",
      "2025-02-20 08:46:45,103 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/satija_IFNB_raw.yaml\n",
      "2025-02-20 08:46:45,105 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/ReplogleWeissman2022_rpe1.yaml\n",
      "2025-02-20 08:46:45,106 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/AdamsonWeissman2016_GSM2406675_10X001.yaml\n",
      "2025-02-20 08:46:45,107 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/DixitRegev2016.yaml\n",
      "2025-02-20 08:46:45,108 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/SrivatsanTrapnell2020_sciplex2.yaml\n",
      "2025-02-20 08:46:45,109 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/NormanWeissman2019_filtered.yaml\n",
      "2025-02-20 08:46:45,111 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/FrangiehIzar2021_protein.yaml\n",
      "2025-02-20 08:46:45,112 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/satija_IFNB_HVG.yaml\n",
      "2025-02-20 08:46:45,113 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/kang.yaml\n",
      "2025-02-20 08:46:45,115 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/DatlingerBock2021.yaml\n",
      "2025-02-20 08:46:45,116 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/ReplogleWeissman2022_K562_gwps.yaml\n",
      "2025-02-20 08:46:45,117 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/essential_gene_knockouts_raw.yaml\n",
      "2025-02-20 08:46:45,119 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/GasperiniShendure2019_highMOI.yaml\n",
      "2025-02-20 08:46:45,120 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/GasperiniShendure2019_lowMOI.yaml\n",
      "2025-02-20 08:46:45,121 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/ZhaoSims2021.yaml\n",
      "2025-02-20 08:46:45,122 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/TianKampmann2019_day7neuron.yaml\n",
      "2025-02-20 08:46:45,123 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/PapalexiSatija2021_eccite_protein.yaml\n",
      "2025-02-20 08:46:45,125 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/SrivatsanTrapnell2020_sciplex3.yaml\n",
      "2025-02-20 08:46:45,126 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/XieHon2017.yaml\n",
      "2025-02-20 08:46:45,127 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/PapalexiSatija2021_eccite_arrayed_protein.yaml\n",
      "2025-02-20 08:46:45,128 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/SchraivogelSteinmetz2020_TAP_SCREEN__chromosome_8_screen.yaml\n",
      "2025-02-20 08:46:45,130 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/McFarlandTsherniak2020.yaml\n",
      "2025-02-20 08:46:45,131 - INFO - Loading training data at path: /orcd/data/omarabu/001/Omnicell_datasets/repogle_k562_essential_raw/K562_essential_raw_singlecell_01.h5ad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 08:46:48,409 - INFO - Loaded unpreprocessed data, # of data points: 310385, # of genes: 8563.\n",
      "2025-02-20 08:46:48,410 - INFO - Preprocessing training data\n",
      "2025-02-20 08:46:48,411 - INFO - Using identity features for perturbations\n",
      "2025-02-20 08:46:48,532 - INFO - Removing observations with perturbations not in the dataset as a column\n",
      "2025-02-20 08:46:48,719 - INFO - Removed 189 perturbations that were not in the dataset columns and 0 perturbations that did not have an embedding for a total of 189 perturbations removed out of an initial 2058 perturbations\n",
      "2025-02-20 08:47:17,709 - INFO - Doing OOD split\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded:\n",
      "- Number of cells: 279630\n",
      "- Input dimension: 8563\n",
      "- Number of perturbations: 1850\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the path to the directory containing the omnicell package\n",
    "# Assuming the omnicell package is in the parent directory of your notebook\n",
    "sys.path.append('..')  # Adjust this path as needed\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from omnicell.config.config import Config, ETLConfig, ModelConfig, DatasplitConfig, EvalConfig, EmbeddingConfig\n",
    "from omnicell.data.loader import DataLoader\n",
    "from omnicell.constants import PERT_KEY, GENE_EMBEDDING_KEY, CONTROL_PERT\n",
    "from omnicell.models.selector import load_model as get_model\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configure paths\n",
    "MODEL_CONFIG = ModelConfig.from_yaml(\"/home/jason497/omnicell/configs/models/autoencoder.yaml\")\n",
    "ETL_CONFIG = ETLConfig(name = \"no_preprocessing\", log1p = False, drop_unmatched_perts = True)\n",
    "EMBEDDING_CONFIG = EmbeddingConfig(pert_embedding='GenePT')\n",
    "\n",
    "SPLIT_CONFIG = DatasplitConfig.from_yaml(\"/orcd/data/omarabu/001/njwfish/omnicell/configs/splits/repogle_k562_essential_raw/random_splits/rs_accP_k562_ood_ss:ns_20_2_most_pert_0.1/split_0/split_config.yaml\")\n",
    "#SPLIT_CONFIG = DatasplitConfig.from_yaml(\"/orcd/data/omarabu/001/njwfish/omnicell/configs/splits/repogle_k562_essential_raw/random_splits/rs_accP_k562_ood_ss:ns_20_2_most_pert_0.1/split_1/split_config.yaml\")\n",
    "EVAL_CONFIG = EvalConfig.from_yaml(\"/orcd/data/omarabu/001/njwfish/omnicell/configs/splits/repogle_k562_essential_raw/random_splits/rs_accP_k562_ood_ss:ns_20_2_most_pert_0.1/split_0/eval_config.yaml\")  # Set this if you want to run evaluations\n",
    "#EVAL_CONFIG = EvalConfig.from_yaml(\"/orcd/data/omarabu/001/njwfish/omnicell/configs/splits/repogle_k562_essential_raw/random_splits/rs_accP_k562_ood_ss:ns_20_2_most_pert_0.1/split_1/eval_config.yaml\")\n",
    "\n",
    "# Load configurations\n",
    "config = Config(model_config=MODEL_CONFIG,\n",
    "                 etl_config=ETL_CONFIG, \n",
    "                 datasplit_config=SPLIT_CONFIG, \n",
    "                 eval_config=EVAL_CONFIG)\n",
    "\n",
    "\n",
    "#Alternatively you can initialize the config objects manually as follows:\n",
    "# etl_config = ETLConfig(name = XXX, log1p = False, drop_unmatched_perts = False, ...)\n",
    "# model_config = ...\n",
    "# embedding_config = ...\n",
    "# datasplit_config = ...\n",
    "# eval_config = ...\n",
    "# config = Config(etl_config, model_config, datasplit_config, eval_config)\n",
    "\n",
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize data loader and load training data\n",
    "loader = DataLoader(config)\n",
    "adata, pert_rep_map = loader.get_training_data()\n",
    "\n",
    "# Get dimensions and perturbation IDs\n",
    "input_dim = adata.shape[1]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pert_ids = adata.obs[PERT_KEY].unique()\n",
    "gene_emb_dim = adata.varm[GENE_EMBEDDING_KEY].shape[1] if GENE_EMBEDDING_KEY in adata.varm else None\n",
    "\n",
    "print(f\"Data loaded:\")\n",
    "print(f\"- Number of cells: {adata.shape[0]}\")\n",
    "print(f\"- Input dimension: {input_dim}\")\n",
    "print(f\"- Number of perturbations: {len(pert_ids)}\")\n",
    "# get index of pert in adata.var_names\n",
    "pert_list = adata.var_names.values.tolist()\n",
    "pert_rep_map_idxs = {pert: pert_list.index(pert) for pert in adata.obs[PERT_KEY].unique() if pert != CONTROL_PERT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.sparse import issparse\n",
    "import scipy\n",
    "import warnings\n",
    "PERT_KEY = 'pert'\n",
    "CELL_KEY = 'cell'\n",
    "CONTROL_PERT = 'ctrl'\n",
    "GENE_VAR_KEY = 'gene_name'\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\",\n",
    "    category=UserWarning\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "##############################################\n",
    "# ADDED FOR KNN (Imports for building and vectorizing)\n",
    "##############################################\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def build_knn_indices(emb_tensor, k=10):\n",
    "    \"\"\"\n",
    "    emb_tensor: torch.Tensor of shape [N, d], on CPU or GPU\n",
    "    Returns: knn_list, a list of arrays where knn_list[i] is the (k) neighbors of i.\n",
    "    \"\"\"\n",
    "    # Move to CPU NumPy for scikit-learn\n",
    "    emb = emb_tensor.detach().cpu().numpy()  # shape [N, d]\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='auto').fit(emb)\n",
    "    distances, indices = nbrs.kneighbors(emb)\n",
    "    \n",
    "    # For each row i, indices[i,0] == i (the point itself)\n",
    "    # So the kNN excluding the point itself is indices[i, 1..k]\n",
    "    knn_list = [row[1:] for row in indices]\n",
    "    return knn_list\n",
    "\n",
    "def build_neighbor_idx(knn_list):\n",
    "    \"\"\"\n",
    "    knn_list: list of length N, where knn_list[i] is a list/array of k neighbors for node i.\n",
    "    Returns a LongTensor neighbor_idx of shape [N, k].\n",
    "    \"\"\"\n",
    "    neighbor_tensors = []\n",
    "    for nbrs in knn_list:\n",
    "        nbrs_t = torch.tensor(nbrs, dtype=torch.long).unsqueeze(0)  # shape [1, k]\n",
    "        neighbor_tensors.append(nbrs_t)\n",
    "    neighbor_idx = torch.cat(neighbor_tensors, dim=0)  # shape [N, k]\n",
    "    return neighbor_idx\n",
    "\n",
    "def knn_pull_loss_vec(emb_tensor, neighbor_idx):\n",
    "    \"\"\"\n",
    "    Vectorized pull loss that encourages each node i to be close to its k neighbors.\n",
    "    emb_tensor: [N, d]\n",
    "    neighbor_idx: [N, k]\n",
    "    Returns: scalar 'pull' loss (MSE between each i and neighbor j).\n",
    "    \"\"\"\n",
    "    device = emb_tensor.device\n",
    "    N, d = emb_tensor.shape\n",
    "    k = neighbor_idx.shape[1]\n",
    "    \n",
    "    # i_idx => shape [N, k]: repeated i for each neighbor\n",
    "    i_idx = torch.arange(N, device=device).unsqueeze(1).expand(N, k)  # [N, k]\n",
    "    \n",
    "    # Gather embeddings\n",
    "    x_i = emb_tensor[i_idx]              # shape [N, k, d]\n",
    "    x_j = emb_tensor[neighbor_idx]       # shape [N, k, d]\n",
    "    \n",
    "    # Squared distances => [N, k, d]\n",
    "    dist_sq = (x_i - x_j)**2\n",
    "    # Sum over the last dimension => [N, k]\n",
    "    dist_sum = dist_sq.sum(dim=2)\n",
    "    # Mean over all pairs => scalar\n",
    "    pull_loss = dist_sum.mean()\n",
    "    return pull_loss\n",
    "\n",
    "##############################################\n",
    "# ADDED FOR NEGATIVE SAMPLING (Push)\n",
    "##############################################\n",
    "def sample_negatives(knn_list, N, num_neg=5000, max_tries=500000):\n",
    "    \"\"\"\n",
    "    Sample 'num_neg' random (i, j) pairs that are:\n",
    "      - i != j\n",
    "      - j not in knn_list[i]\n",
    "    We do random i, j until we collect 'num_neg' valid pairs or exceed 'max_tries'.\n",
    "    \n",
    "    Returns two LongTensors i_neg, j_neg of shape [num_neg].\n",
    "    \"\"\"\n",
    "    # Build a set of neighbors for quick membership checks\n",
    "    neighbors_set = []\n",
    "    for i, nbrs in enumerate(knn_list):\n",
    "        neighbors_set.append(set(nbrs.tolist()))\n",
    "    \n",
    "    i_neg_list = []\n",
    "    j_neg_list = []\n",
    "    tries = 0\n",
    "    \n",
    "    while len(i_neg_list) < num_neg and tries < max_tries:\n",
    "        i_ = np.random.randint(0, N)\n",
    "        j_ = np.random.randint(0, N)\n",
    "        if j_ == i_:\n",
    "            tries += 1\n",
    "            continue\n",
    "        if j_ in neighbors_set[i_]:\n",
    "            tries += 1\n",
    "            continue\n",
    "\n",
    "        # valid negative pair\n",
    "        i_neg_list.append(i_)\n",
    "        j_neg_list.append(j_)\n",
    "        tries += 1\n",
    "    \n",
    "    # If we didn't get enough pairs, replicate\n",
    "    if len(i_neg_list) < num_neg:\n",
    "        shortfall = num_neg - len(i_neg_list)\n",
    "        i_neg_list += i_neg_list[:shortfall]\n",
    "        j_neg_list += j_neg_list[:shortfall]\n",
    "    \n",
    "    i_neg_t = torch.tensor(i_neg_list[:num_neg], dtype=torch.long)\n",
    "    j_neg_t = torch.tensor(j_neg_list[:num_neg], dtype=torch.long)\n",
    "    return i_neg_t, j_neg_t\n",
    "\n",
    "def neg_push_loss(emb_tensor, i_neg, j_neg, margin=1.0):\n",
    "    \"\"\"\n",
    "    Margin-based push loss for negative pairs (i_neg, j_neg).\n",
    "    If dist(x_i, x_j) < margin, we penalize => max(0, margin - dist).\n",
    "    \n",
    "    emb_tensor: [N, d]\n",
    "    i_neg, j_neg: shape [num_neg]\n",
    "    margin: float\n",
    "    Returns a scalar push loss\n",
    "    \"\"\"\n",
    "    device = emb_tensor.device\n",
    "    x_i = emb_tensor[i_neg]  # shape [num_neg, d]\n",
    "    x_j = emb_tensor[j_neg]  # shape [num_neg, d]\n",
    "    \n",
    "    # L2 distance\n",
    "    dist = torch.sqrt(((x_i - x_j)**2).sum(dim=1) + 1e-8)  # shape [num_neg]\n",
    "    \n",
    "    # margin-based hinge\n",
    "    push = F.relu(margin - dist)  # shape [num_neg]\n",
    "    return push.mean()\n",
    "\n",
    "\n",
    "class SinusoidalIntegerApprox(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified integer approximation that maintains non-zero gradients at integers\n",
    "    while still encouraging integer-like outputs\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.1):\n",
    "        super().__init__()\n",
    "        self.two_pi = 2 * np.pi\n",
    "        self.alpha = alpha  # Controls how much gradient to maintain at integers\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Original term encourages integer values\n",
    "        integer_term = x - torch.sin(self.two_pi * x) / self.two_pi\n",
    "        \n",
    "        # Add small linear term to maintain gradient\n",
    "        # This prevents gradient from going completely to zero at integers\n",
    "        return (1 - self.alpha) * integer_term + self.alpha * x\n",
    "\n",
    "class SmoothPositiveIntegerApprox(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom activation function that approximates positive integers with smooth tailing\n",
    "    Uses a modified version of the sinusoidal approximation that:\n",
    "    1. Smoothly approaches positive values using softplus\n",
    "    2. Has diminishing oscillations at higher values\n",
    "    3. Maintains gradients throughout the input range\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.1, beta=2):\n",
    "        super().__init__()\n",
    "        self.two_pi = 2 * np.pi\n",
    "        self.beta = beta  # Controls the sharpness of the softplus transition\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Smooth positive transformation using softplus\n",
    "        x_pos = F.softplus(x, beta=self.beta)\n",
    "        \n",
    "        # Modified sinusoidal term with diminishing effect\n",
    "        sin_term = torch.sin(self.two_pi * x_pos) / self.two_pi\n",
    "        integer_term = x_pos - sin_term\n",
    "        \n",
    "        return (1 - self.alpha) * integer_term + self.alpha * x_pos\n",
    "    \n",
    "class autoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Architecture:\n",
    "      1) control_encoder: (b, G) => (b, enc_dim)\n",
    "      2) pert_embedding: (b,) => (b, enc_dim)\n",
    "      3) gene_embedding: (G, enc_dim)\n",
    "      4) For each gene => combine => decode to produce (b, G) for:\n",
    "         - pred_ctrl\n",
    "         - pred_delta\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_config: dict,\n",
    "        num_genes: int,\n",
    "        enc_dim_cell: int = 340,\n",
    "        enc_dim_pert: int = 80,\n",
    "        hidden_enc_1: int = 500,\n",
    "        hidden_dec_1: int = 340\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model=None\n",
    "        self.num_genes = num_genes\n",
    "\n",
    "        # Encode Control\n",
    "\n",
    "        # Pert Embedding\n",
    "        self.shared_embedding = nn.Embedding(num_genes, enc_dim_pert)\n",
    "\n",
    "        # We'll decode for pred_ctrl and pred_delta using\n",
    "        # a single hidden layer, but then 2 heads:\n",
    "        #   head_ctrl => (hidden_dec_1->1)\n",
    "        #   head_delta => (hidden_dec_1->1)\n",
    "        self.hidden_layer = nn.Linear(2*enc_dim_pert+num_genes, hidden_enc_1)\n",
    "        self.output_layer = nn.Linear(hidden_enc_1, 1)\n",
    "\n",
    "    def forward(self, x_ctrl_log, whichpert_idx, multiplier):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          x_ctrl_log: (b, G)\n",
    "          whichpert_idx: (b,)\n",
    "        Returns:\n",
    "          pred_ctrl:  (b, G)\n",
    "          pred_delta: (b, G)\n",
    "        \"\"\"\n",
    "        \n",
    "        #forward\n",
    "        b, G = x_ctrl_log.size()\n",
    "\n",
    "\n",
    "        # Gene embeddings => shape(G, enc_dim_pert)\n",
    "        gene_emb_all = self.shared_embedding.weight[:G]\n",
    "        ge_expanded  = gene_emb_all.unsqueeze(0).expand(b, G, -1)\n",
    "        pert_embed = self.shared_embedding(whichpert_idx)\n",
    "        pert_embed_flat = pert_embed.unsqueeze(1).expand(b, G, -1)\n",
    "        x_ctrl_log_flat = x_ctrl_log.unsqueeze(1).expand(b, G, -1)\n",
    "        \n",
    "        \n",
    "        # Concat => shape(b, G, enc_dim_pert*2 + enc_dim_cell)\n",
    "        dec_in = torch.cat([x_ctrl_log_flat, pert_embed_flat, ge_expanded], dim=2)\n",
    "        # Flatten => shape(b*G, enc_dim_pert*2 + enc_dim_cell)\n",
    "        dec_in_flat = dec_in.view(b*G, -1)\n",
    "        \n",
    "        x_hidden = F.relu(self.hidden_layer(dec_in_flat))\n",
    "        pred_delta = F.relu(self.output_layer(x_hidden)).view(b, G)\n",
    "\n",
    "        return pred_delta\n",
    "\n",
    "    def log_normalized_mse_dual(self, pred_delta, true_ctrl, true_pert, normalize_to=10000):\n",
    "        \"\"\"\n",
    "        Weighted MSE over:\n",
    "           - pred_ctrl vs. true_ctrl\n",
    "           - pred_delta vs. (true_pert - true_ctrl)\n",
    "        Using log1p(...) of (counts / normalized_sum).\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        true_delta = true_pert - true_ctrl\n",
    "\n",
    "        diff_sq_delta = (pred_delta - true_delta)**2\n",
    "\n",
    "        loss_delta = diff_sq_delta.mean()\n",
    "\n",
    "        return loss_delta\n",
    "\n",
    "    def mse_dual(self, pred_ctrl, pred_delta, true_ctrl, true_pert):\n",
    "        \"\"\"\n",
    "        Weighted MSE over:\n",
    "          - pred_ctrl vs. true_ctrl\n",
    "          - pred_delta vs. (true_pert - true_ctrl)\n",
    "        \"\"\"\n",
    "        true_delta = true_pert - true_ctrl\n",
    "        diff_sq_ctrl  = (pred_ctrl - true_ctrl)**2\n",
    "        diff_sq_delta = (pred_delta - true_delta)**2\n",
    "\n",
    "        loss_ctrl  = diff_sq_ctrl.mean()\n",
    "        loss_delta = diff_sq_delta.mean()\n",
    "        loss_total = 0.9*loss_ctrl + 0.1*loss_delta\n",
    "        return loss_total, loss_ctrl.item(), loss_delta.item()\n",
    "\n",
    "    def train(self, dl, lr=1e-3):\n",
    "        \"\"\"\n",
    "        Simple custom training loop that runs a single epoch \n",
    "        (as per the original notebook logic).\n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "        \n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        start_epoch = 0\n",
    "        num_epochs  = 1\n",
    "        print_interval = 500\n",
    "        \n",
    "        KNN_K = 10\n",
    "        KNN_EMB_LOSS_WEIGHT = 1e-2\n",
    "        REBUILD_EVERY = 2\n",
    "        NEG_LOSS_WEIGHT = 1e-2\n",
    "        NEG_PAIRS = 7000\n",
    "        MARGIN = 1.0\n",
    "        \n",
    "        neg_i = None\n",
    "        neg_j = None\n",
    "        knn_list = None\n",
    "        neighbor_idx = None\n",
    "        \n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            # Rebuild adjacency (knn_list) + neighbor_idx + negative pairs\n",
    "            if epoch % REBUILD_EVERY == 0:\n",
    "                emb_mat = self.shared_embedding.weight  # shape [N, d]\n",
    "                N = emb_mat.size(0)\n",
    "        \n",
    "                knn_list = build_knn_indices(emb_mat, k=KNN_K)\n",
    "                neighbor_idx = build_neighbor_idx(knn_list).to(emb_mat.device)\n",
    "        \n",
    "                i_neg_t, j_neg_t = sample_negatives(knn_list, N, num_neg=NEG_PAIRS)\n",
    "                neg_i = i_neg_t.to(emb_mat.device)\n",
    "                neg_j = j_neg_t.to(emb_mat.device)\n",
    "        \n",
    "            # Reset accumulators for each printing interval\n",
    "            running_loss_total = 0.0\n",
    "            running_loss_ctrl  = 0.0\n",
    "            running_loss_delta = 0.0\n",
    "            running_loss_knn   = 0.0\n",
    "            running_loss_neg   = 0.0\n",
    "            num_samples = 0\n",
    "        \n",
    "            for batch_idx, (x_ctrl_batch_in, x_pert_batch_in, whichpert_batch_in) in enumerate(dl):\n",
    "                x_ctrl_batch = x_ctrl_batch_in.to(device)\n",
    "                x_pert_batch = x_pert_batch_in.to(device)\n",
    "                whichpert_batch = torch.clone(whichpert_batch_in).to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "                pred_delta = self.forward(x_ctrl_batch, whichpert_batch, multiplier=1)\n",
    "        \n",
    "                # Get losses from your dual loss function:\n",
    "                loss_total = self.log_normalized_mse_dual(\n",
    "                    pred_delta, x_ctrl_batch, x_pert_batch\n",
    "                )\n",
    "        \n",
    "                # Compute the knn pull loss:\n",
    "                if neighbor_idx is not None:\n",
    "                    emb_mat = self.shared_embedding.weight\n",
    "                    loss_knn = knn_pull_loss_vec(emb_mat, neighbor_idx)\n",
    "                else:\n",
    "                    loss_knn = 0.0\n",
    "        \n",
    "                # Compute the negative sampling push loss:\n",
    "                if neg_i is not None and neg_j is not None:\n",
    "                    emb_mat = self.shared_embedding.weight\n",
    "                    dist_push = neg_push_loss(emb_mat, neg_i, neg_j, margin=MARGIN)\n",
    "                    loss_neg = dist_push\n",
    "                else:\n",
    "                    loss_neg = 0.0\n",
    "        \n",
    "                # Combine losses into the final loss:\n",
    "                loss_total_with_knn = loss_total\n",
    "                if loss_knn != 0.0:\n",
    "                    loss_total_with_knn += KNN_EMB_LOSS_WEIGHT * loss_knn\n",
    "                if loss_neg != 0.0:\n",
    "                    loss_total_with_knn += NEG_LOSS_WEIGHT * loss_neg\n",
    "        \n",
    "                loss_total_with_knn.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "                bs = x_ctrl_batch.size(0)\n",
    "                running_loss_total += loss_total.item() * bs\n",
    "                running_loss_knn   += (loss_knn.item() if hasattr(loss_knn, 'item') else loss_knn) * bs\n",
    "                running_loss_neg   += (loss_neg.item() if hasattr(loss_neg, 'item') else loss_neg) * bs\n",
    "                num_samples += bs\n",
    "        \n",
    "                iteration = batch_idx + 1 + epoch * len(dl)\n",
    "                if iteration % print_interval == 0:\n",
    "                    curr_total = running_loss_total / num_samples\n",
    "                    curr_ctrl  = running_loss_ctrl  / num_samples\n",
    "                    curr_delta = running_loss_delta / num_samples\n",
    "                    curr_knn   = running_loss_knn   / num_samples\n",
    "                    curr_neg   = running_loss_neg   / num_samples\n",
    "        \n",
    "                    elapsed = time.time() - start_time\n",
    "                    print(f\"[Epoch {epoch+1}, Iter {batch_idx+1}] \"\n",
    "                          f\"Train MSE Total={curr_total:.6f} | \"\n",
    "                          f\"knn_loss={curr_knn:.6f} | neg_loss={curr_neg:.6f}   \"\n",
    "                          f\"Time for last {print_interval} iters: {elapsed:.2f}s\")\n",
    "                    \n",
    "                    # Reset the accumulators and timer\n",
    "                    running_loss_total = 0.0\n",
    "                    running_loss_ctrl  = 0.0\n",
    "                    running_loss_delta = 0.0\n",
    "                    running_loss_knn   = 0.0\n",
    "                    running_loss_neg   = 0.0\n",
    "                    num_samples = 0\n",
    "                    start_time = time.time()\n",
    "\n",
    "    def make_predict(self, adata: sc.AnnData, pert_id: str, cell_type: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Inference routine: given (control) adata for a specific cell_type,\n",
    "        produce predicted counts for the given pert_id.\n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "\n",
    "        # Extract control cells for the given cell_type\n",
    "        ctrl_cells_adata = adata[\n",
    "            (adata.obs[PERT_KEY] == CONTROL_PERT) & (adata.obs[CELL_KEY] == cell_type)\n",
    "        ]\n",
    "        if ctrl_cells_adata.shape[0] == 0:\n",
    "            # If no such cells exist, return an empty array\n",
    "            logger.warning(f\"No control cells found for cell_type={cell_type}. Returning empty array.\")\n",
    "            return np.array([])\n",
    "\n",
    "        ctrl_cells = ctrl_cells_adata.X.toarray().copy()\n",
    "        ctrl_cells = torch.from_numpy(ctrl_cells).float().to(device)\n",
    "        ctrl_cells = torch.log1p(ctrl_cells)\n",
    "\n",
    "        # Build whichpert_idx\n",
    "        # We rely on adata.var[GENE_VAR_KEY] to find index of pert_id\n",
    "        if pert_id not in adata.var[GENE_VAR_KEY].values:\n",
    "            logger.warning(f\"Pert_id={pert_id} not in adata.var. Returning empty.\")\n",
    "            return np.array([])\n",
    "\n",
    "        whichpert_idx = np.where(adata.var[GENE_VAR_KEY] == pert_id)[0][0]\n",
    "        whichpert = torch.tensor([whichpert_idx] * ctrl_cells.shape[0], dtype=torch.long, device=device)\n",
    "\n",
    "        batch_size = 32\n",
    "        pred_delta_list = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, ctrl_cells.size(0), batch_size):\n",
    "                batch_ctrl_cells = ctrl_cells[i:i+batch_size]\n",
    "                batch_whichpert  = whichpert[i:i+batch_size]\n",
    "                batch_pred_delta = self.forward(batch_ctrl_cells, batch_whichpert, multiplier=1)\n",
    "                pred_delta_list.append(batch_pred_delta.clone())\n",
    "\n",
    "        pred_delta = torch.cat(pred_delta_list, dim=0)\n",
    "        pred = ctrl_cells + pred_delta\n",
    "        pred[pred <= 0] = 0  # clamp negative predictions to 0\n",
    "        pred = pred.cpu().detach().numpy()\n",
    "        pred = np.round(pred)\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['k562']\n",
      "['AAAS' 'AAMP' 'AARS' ... 'ZRSR2' 'ZW10' 'ZWINT']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "npz = np.load(\"/orcd/data/omarabu/001/Omnicell_datasets/repogle_k562_essential_raw/proportional_scot/synthetic_counterfactuals_0.pkl\", allow_pickle=True)\n",
    "\n",
    "class PairedStratifiedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "            self, source_dict, target_dict, pert_map\n",
    "    ):\n",
    "        self.source = source_dict\n",
    "        self.target = target_dict\n",
    "        self.strata = np.array(list(self.source.keys()))\n",
    "        print(self.strata)\n",
    "        self.unique_pert_ids = np.array(list(self.target[self.strata[0]].keys()))\n",
    "        print(self.unique_pert_ids)\n",
    "        self.pert_map = pert_map\n",
    "        self.ns = np.array([\n",
    "            len(self.source[stratum]) for stratum in self.strata\n",
    "        ])\n",
    "\n",
    "        self.samples_per_epoch = len(self.unique_pert_ids) * self.source[self.strata[0]].shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_dict * self.target_dict)\n",
    "    \n",
    "    def __getitem__(self, strata_idx):\n",
    "        (stratum_idx,), idx = strata_idx\n",
    "        stratum = self.strata[stratum_idx]\n",
    "        pert = np.random.choice(self.unique_pert_ids)\n",
    "        return (\n",
    "            self.source[stratum][idx],\n",
    "            self.target[stratum][pert][idx],\n",
    "            self.pert_map[pert]\n",
    "        )\n",
    "    \n",
    "dset = PairedStratifiedDataset(\n",
    "    source_dict=npz['source'],\n",
    "    target_dict=npz['synthetic_counterfactuals'],\n",
    "    pert_map=pert_rep_map_idxs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omnicell.models.utils.datamodules import StratifiedBatchSampler, StreamingOfflinePairedStratifiedDataset\n",
    "\n",
    "load_all_data = False\n",
    "if load_all_data:\n",
    "    dset = StreamingOfflinePairedStratifiedDataset(\n",
    "        data_dir='/orcd/data/omarabu/001/Omnicell_datasets/repogle_k562_essential_raw/proportional_scot',\n",
    "        pert_embedding=pert_rep_map_idxs,\n",
    "        num_files=21,\n",
    "        device='cuda'\n",
    "    )\n",
    "\n",
    "dl = torch.utils.data.DataLoader(\n",
    "    dset, \n",
    "    batch_sampler=StratifiedBatchSampler(\n",
    "        ns=dset.ns, batch_size=16, samples_per_epoch=dset.samples_per_epoch\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoencoder(None, 8563)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Iter 500] Train MSE so far=0.897812 (ctrl+delta)   Time last 500 iters: 7.37s\n",
      "[Epoch 1, Iter 1000] Train MSE so far=0.449795 (ctrl+delta)   Time last 500 iters: 6.84s\n",
      "[Epoch 1, Iter 1500] Train MSE so far=0.287755 (ctrl+delta)   Time last 500 iters: 6.85s\n",
      "[Epoch 1, Iter 2000] Train MSE so far=0.203946 (ctrl+delta)   Time last 500 iters: 6.86s\n",
      "[Epoch 1, Iter 2500] Train MSE so far=0.163224 (ctrl+delta)   Time last 500 iters: 6.86s\n",
      "[Epoch 1, Iter 3000] Train MSE so far=0.138004 (ctrl+delta)   Time last 500 iters: 6.99s\n",
      "[Epoch 1, Iter 3500] Train MSE so far=0.124729 (ctrl+delta)   Time last 500 iters: 6.96s\n",
      "[Epoch 1, Iter 4000] Train MSE so far=0.118541 (ctrl+delta)   Time last 500 iters: 6.97s\n",
      "[Epoch 1, Iter 4500] Train MSE so far=0.115352 (ctrl+delta)   Time last 500 iters: 6.95s\n",
      "[Epoch 1, Iter 5000] Train MSE so far=0.113665 (ctrl+delta)   Time last 500 iters: 6.96s\n",
      "[Epoch 1, Iter 5500] Train MSE so far=0.109977 (ctrl+delta)   Time last 500 iters: 6.95s\n",
      "[Epoch 1, Iter 6000] Train MSE so far=0.110796 (ctrl+delta)   Time last 500 iters: 6.94s\n",
      "[Epoch 1, Iter 6500] Train MSE so far=0.108882 (ctrl+delta)   Time last 500 iters: 6.95s\n",
      "[Epoch 1, Iter 7000] Train MSE so far=0.108185 (ctrl+delta)   Time last 500 iters: 6.95s\n",
      "[Epoch 1, Iter 7500] Train MSE so far=0.108724 (ctrl+delta)   Time last 500 iters: 6.97s\n",
      "[Epoch 1, Iter 8000] Train MSE so far=0.127569 (ctrl+delta)   Time last 500 iters: 6.95s\n",
      "[Epoch 1, Iter 8500] Train MSE so far=0.125790 (ctrl+delta)   Time last 500 iters: 6.95s\n",
      "[Epoch 1, Iter 9000] Train MSE so far=0.125820 (ctrl+delta)   Time last 500 iters: 6.95s\n",
      "[Epoch 1, Iter 9500] Train MSE so far=0.126680 (ctrl+delta)   Time last 500 iters: 6.95s\n",
      "[Epoch 1, Iter 10000] Train MSE so far=0.125504 (ctrl+delta)   Time last 500 iters: 6.94s\n",
      "[Epoch 1, Iter 10500] Train MSE so far=0.126881 (ctrl+delta)   Time last 500 iters: 6.94s\n",
      "[Epoch 1, Iter 11000] Train MSE so far=0.126540 (ctrl+delta)   Time last 500 iters: 6.94s\n",
      "[Epoch 1, Iter 11500] Train MSE so far=0.125168 (ctrl+delta)   Time last 500 iters: 6.95s\n",
      "[Epoch 1, Iter 12000] Train MSE so far=0.126614 (ctrl+delta)   Time last 500 iters: 6.94s\n",
      "[Epoch 1, Iter 12500] Train MSE so far=0.126165 (ctrl+delta)   Time last 500 iters: 6.96s\n",
      "[Epoch 1, Iter 13000] Train MSE so far=0.126317 (ctrl+delta)   Time last 500 iters: 6.98s\n",
      "[Epoch 1, Iter 13500] Train MSE so far=0.124441 (ctrl+delta)   Time last 500 iters: 6.96s\n",
      "[Epoch 1, Iter 14000] Train MSE so far=0.126784 (ctrl+delta)   Time last 500 iters: 6.95s\n",
      "[Epoch 1, Iter 14500] Train MSE so far=0.125567 (ctrl+delta)   Time last 500 iters: 6.94s\n",
      "[Epoch 1, Iter 15000] Train MSE so far=0.124762 (ctrl+delta)   Time last 500 iters: 6.94s\n",
      "[Epoch 1, Iter 15500] Train MSE so far=0.125096 (ctrl+delta)   Time last 500 iters: 6.94s\n",
      "[Epoch 1, Iter 16000] Train MSE so far=0.124572 (ctrl+delta)   Time last 500 iters: 6.95s\n",
      "[Epoch 1, Iter 16500] Train MSE so far=0.126027 (ctrl+delta)   Time last 500 iters: 6.94s\n",
      "[Epoch 1, Iter 17000] Train MSE so far=0.125658 (ctrl+delta)   Time last 500 iters: 6.94s\n",
      "[Epoch 1, Iter 17500] Train MSE so far=0.125640 (ctrl+delta)   Time last 500 iters: 6.96s\n",
      "[Epoch 1, Iter 18000] Train MSE so far=0.125203 (ctrl+delta)   Time last 500 iters: 6.97s\n",
      "[Epoch 1, Iter 18500] Train MSE so far=0.126098 (ctrl+delta)   Time last 500 iters: 6.95s\n",
      "[Epoch 1, Iter 19000] Train MSE so far=0.125419 (ctrl+delta)   Time last 500 iters: 6.94s\n",
      "[Epoch 1, Iter 19500] Train MSE so far=0.124035 (ctrl+delta)   Time last 500 iters: 6.94s\n",
      "[Epoch 1, Iter 20000] Train MSE so far=0.124712 (ctrl+delta)   Time last 500 iters: 6.94s\n",
      "[Epoch 1, Iter 20500] Train MSE so far=0.126081 (ctrl+delta)   Time last 500 iters: 6.94s\n",
      "[Epoch 1, Iter 21000] Train MSE so far=0.125015 (ctrl+delta)   Time last 500 iters: 6.94s\n",
      "[Epoch 1, Iter 21500] Train MSE so far=0.125862 (ctrl+delta)   Time last 500 iters: 6.95s\n",
      "[Epoch 1, Iter 22000] Train MSE so far=0.125014 (ctrl+delta)   Time last 500 iters: 6.97s\n",
      "[Epoch 1, Iter 22500] Train MSE so far=0.124908 (ctrl+delta)   Time last 500 iters: 6.94s\n",
      "[Epoch 1, Iter 23000] Train MSE so far=0.125044 (ctrl+delta)   Time last 500 iters: 6.94s\n",
      "[Epoch 1, Iter 23500] Train MSE so far=0.125154 (ctrl+delta)   Time last 500 iters: 6.95s\n",
      "[Epoch 1, Iter 24000] Train MSE so far=0.125492 (ctrl+delta)   Time last 500 iters: 6.95s\n",
      "[Epoch 1, Iter 24500] Train MSE so far=0.126019 (ctrl+delta)   Time last 500 iters: 6.99s\n",
      "[Epoch 1, Iter 25000] Train MSE so far=0.125018 (ctrl+delta)   Time last 500 iters: 6.99s\n",
      "[Epoch 1, Iter 25500] Train MSE so far=0.125514 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 26000] Train MSE so far=0.124100 (ctrl+delta)   Time last 500 iters: 6.99s\n",
      "[Epoch 1, Iter 26500] Train MSE so far=0.124112 (ctrl+delta)   Time last 500 iters: 6.98s\n",
      "[Epoch 1, Iter 27000] Train MSE so far=0.125008 (ctrl+delta)   Time last 500 iters: 7.03s\n",
      "[Epoch 1, Iter 27500] Train MSE so far=0.124679 (ctrl+delta)   Time last 500 iters: 6.99s\n",
      "[Epoch 1, Iter 28000] Train MSE so far=0.124370 (ctrl+delta)   Time last 500 iters: 6.98s\n",
      "[Epoch 1, Iter 28500] Train MSE so far=0.125326 (ctrl+delta)   Time last 500 iters: 6.98s\n",
      "[Epoch 1, Iter 29000] Train MSE so far=0.124631 (ctrl+delta)   Time last 500 iters: 6.98s\n",
      "[Epoch 1, Iter 29500] Train MSE so far=0.124713 (ctrl+delta)   Time last 500 iters: 6.98s\n",
      "[Epoch 1, Iter 30000] Train MSE so far=0.124471 (ctrl+delta)   Time last 500 iters: 6.98s\n",
      "[Epoch 1, Iter 30500] Train MSE so far=0.124128 (ctrl+delta)   Time last 500 iters: 6.98s\n",
      "[Epoch 1, Iter 31000] Train MSE so far=0.124546 (ctrl+delta)   Time last 500 iters: 6.98s\n",
      "[Epoch 1, Iter 31500] Train MSE so far=0.125949 (ctrl+delta)   Time last 500 iters: 6.98s\n",
      "[Epoch 1, Iter 32000] Train MSE so far=0.124536 (ctrl+delta)   Time last 500 iters: 6.98s\n",
      "[Epoch 1, Iter 32500] Train MSE so far=0.125090 (ctrl+delta)   Time last 500 iters: 6.98s\n",
      "[Epoch 1, Iter 33000] Train MSE so far=0.125617 (ctrl+delta)   Time last 500 iters: 6.98s\n",
      "[Epoch 1, Iter 33500] Train MSE so far=0.125723 (ctrl+delta)   Time last 500 iters: 6.98s\n",
      "[Epoch 1, Iter 34000] Train MSE so far=0.124307 (ctrl+delta)   Time last 500 iters: 6.98s\n",
      "[Epoch 1, Iter 34500] Train MSE so far=0.125071 (ctrl+delta)   Time last 500 iters: 6.98s\n",
      "[Epoch 1, Iter 35000] Train MSE so far=0.125033 (ctrl+delta)   Time last 500 iters: 6.99s\n",
      "[Epoch 1, Iter 35500] Train MSE so far=0.125276 (ctrl+delta)   Time last 500 iters: 6.99s\n",
      "[Epoch 1, Iter 36000] Train MSE so far=0.124446 (ctrl+delta)   Time last 500 iters: 6.98s\n",
      "[Epoch 1, Iter 36500] Train MSE so far=0.123833 (ctrl+delta)   Time last 500 iters: 6.99s\n",
      "[Epoch 1, Iter 37000] Train MSE so far=0.125020 (ctrl+delta)   Time last 500 iters: 6.99s\n",
      "[Epoch 1, Iter 37500] Train MSE so far=0.125135 (ctrl+delta)   Time last 500 iters: 6.98s\n",
      "[Epoch 1, Iter 38000] Train MSE so far=0.124082 (ctrl+delta)   Time last 500 iters: 6.99s\n",
      "[Epoch 1, Iter 38500] Train MSE so far=0.123997 (ctrl+delta)   Time last 500 iters: 6.99s\n",
      "[Epoch 1, Iter 39000] Train MSE so far=0.124194 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 39500] Train MSE so far=0.124861 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 40000] Train MSE so far=0.123848 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 40500] Train MSE so far=0.124704 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 41000] Train MSE so far=0.123935 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 41500] Train MSE so far=0.123168 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 42000] Train MSE so far=0.124048 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 42500] Train MSE so far=0.123739 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 43000] Train MSE so far=0.124272 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 43500] Train MSE so far=0.123760 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 44000] Train MSE so far=0.123926 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 44500] Train MSE so far=0.124361 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 45000] Train MSE so far=0.125493 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 45500] Train MSE so far=0.124709 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 46000] Train MSE so far=0.124304 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 46500] Train MSE so far=0.124838 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 47000] Train MSE so far=0.125053 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 47500] Train MSE so far=0.124276 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 48000] Train MSE so far=0.124236 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 48500] Train MSE so far=0.125132 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 49000] Train MSE so far=0.125184 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 49500] Train MSE so far=0.124992 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 50000] Train MSE so far=0.125525 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 50500] Train MSE so far=0.124016 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 51000] Train MSE so far=0.124044 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 51500] Train MSE so far=0.124723 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 52000] Train MSE so far=0.124592 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 52500] Train MSE so far=0.124385 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 53000] Train MSE so far=0.124226 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 53500] Train MSE so far=0.124755 (ctrl+delta)   Time last 500 iters: 7.01s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 452\u001b[0m, in \u001b[0;36mautoencoder.train\u001b[0;34m(self, dl, lr)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss_neg \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    450\u001b[0m     loss_total_with_knn \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m NEG_LOSS_WEIGHT \u001b[38;5;241m*\u001b[39m loss_neg\n\u001b[0;32m--> 452\u001b[0m \u001b[43mloss_total_with_knn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    455\u001b[0m bs_ \u001b[38;5;241m=\u001b[39m x_ctrl_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/omnicell/lib/python3.9/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/omnicell/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/omnicell/lib/python3.9/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(dl, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Iter 500] Train MSE so far=0.114817 (ctrl+delta)   Time last 500 iters: 7.11s\n",
      "[Epoch 1, Iter 1000] Train MSE so far=0.114359 (ctrl+delta)   Time last 500 iters: 7.02s\n",
      "[Epoch 1, Iter 1500] Train MSE so far=0.113917 (ctrl+delta)   Time last 500 iters: 7.07s\n",
      "[Epoch 1, Iter 2000] Train MSE so far=0.113912 (ctrl+delta)   Time last 500 iters: 7.06s\n",
      "[Epoch 1, Iter 2500] Train MSE so far=0.114046 (ctrl+delta)   Time last 500 iters: 7.06s\n",
      "[Epoch 1, Iter 3000] Train MSE so far=0.114259 (ctrl+delta)   Time last 500 iters: 7.05s\n",
      "[Epoch 1, Iter 3500] Train MSE so far=0.113569 (ctrl+delta)   Time last 500 iters: 7.05s\n",
      "[Epoch 1, Iter 4000] Train MSE so far=0.113675 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 4500] Train MSE so far=0.113463 (ctrl+delta)   Time last 500 iters: 7.05s\n",
      "[Epoch 1, Iter 5000] Train MSE so far=0.113553 (ctrl+delta)   Time last 500 iters: 7.06s\n",
      "[Epoch 1, Iter 5500] Train MSE so far=0.113685 (ctrl+delta)   Time last 500 iters: 7.05s\n",
      "[Epoch 1, Iter 6000] Train MSE so far=0.113544 (ctrl+delta)   Time last 500 iters: 7.05s\n",
      "[Epoch 1, Iter 6500] Train MSE so far=0.113617 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 7000] Train MSE so far=0.113747 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 7500] Train MSE so far=0.113613 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 8000] Train MSE so far=0.113062 (ctrl+delta)   Time last 500 iters: 7.05s\n",
      "[Epoch 1, Iter 8500] Train MSE so far=0.113363 (ctrl+delta)   Time last 500 iters: 7.05s\n",
      "[Epoch 1, Iter 9000] Train MSE so far=0.113085 (ctrl+delta)   Time last 500 iters: 7.05s\n",
      "[Epoch 1, Iter 9500] Train MSE so far=0.113426 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 10000] Train MSE so far=0.113752 (ctrl+delta)   Time last 500 iters: 7.06s\n",
      "[Epoch 1, Iter 10500] Train MSE so far=0.113254 (ctrl+delta)   Time last 500 iters: 7.05s\n",
      "[Epoch 1, Iter 11000] Train MSE so far=0.113089 (ctrl+delta)   Time last 500 iters: 7.05s\n",
      "[Epoch 1, Iter 11500] Train MSE so far=0.113356 (ctrl+delta)   Time last 500 iters: 7.02s\n",
      "[Epoch 1, Iter 12000] Train MSE so far=0.113624 (ctrl+delta)   Time last 500 iters: 7.03s\n",
      "[Epoch 1, Iter 12500] Train MSE so far=0.113566 (ctrl+delta)   Time last 500 iters: 7.06s\n",
      "[Epoch 1, Iter 13000] Train MSE so far=0.113387 (ctrl+delta)   Time last 500 iters: 7.03s\n",
      "[Epoch 1, Iter 13500] Train MSE so far=0.113734 (ctrl+delta)   Time last 500 iters: 7.03s\n",
      "[Epoch 1, Iter 14000] Train MSE so far=0.113354 (ctrl+delta)   Time last 500 iters: 7.05s\n",
      "[Epoch 1, Iter 14500] Train MSE so far=0.113949 (ctrl+delta)   Time last 500 iters: 7.05s\n",
      "[Epoch 1, Iter 15000] Train MSE so far=0.113419 (ctrl+delta)   Time last 500 iters: 7.05s\n",
      "[Epoch 1, Iter 15500] Train MSE so far=0.114003 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 16000] Train MSE so far=0.113421 (ctrl+delta)   Time last 500 iters: 7.03s\n",
      "[Epoch 1, Iter 16500] Train MSE so far=0.113647 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 17000] Train MSE so far=0.113455 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 17500] Train MSE so far=0.113420 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 18000] Train MSE so far=0.114256 (ctrl+delta)   Time last 500 iters: 7.05s\n",
      "[Epoch 1, Iter 18500] Train MSE so far=0.113019 (ctrl+delta)   Time last 500 iters: 7.05s\n",
      "[Epoch 1, Iter 19000] Train MSE so far=0.113575 (ctrl+delta)   Time last 500 iters: 7.05s\n",
      "[Epoch 1, Iter 19500] Train MSE so far=0.113483 (ctrl+delta)   Time last 500 iters: 7.06s\n",
      "[Epoch 1, Iter 20000] Train MSE so far=0.113211 (ctrl+delta)   Time last 500 iters: 7.15s\n",
      "[Epoch 1, Iter 20500] Train MSE so far=0.113277 (ctrl+delta)   Time last 500 iters: 7.07s\n",
      "[Epoch 1, Iter 21000] Train MSE so far=0.113683 (ctrl+delta)   Time last 500 iters: 7.06s\n",
      "[Epoch 1, Iter 21500] Train MSE so far=0.113441 (ctrl+delta)   Time last 500 iters: 7.16s\n",
      "[Epoch 1, Iter 22000] Train MSE so far=0.113277 (ctrl+delta)   Time last 500 iters: 7.09s\n",
      "[Epoch 1, Iter 22500] Train MSE so far=0.113548 (ctrl+delta)   Time last 500 iters: 7.07s\n",
      "[Epoch 1, Iter 23000] Train MSE so far=0.113535 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 23500] Train MSE so far=0.113605 (ctrl+delta)   Time last 500 iters: 7.06s\n",
      "[Epoch 1, Iter 24000] Train MSE so far=0.113275 (ctrl+delta)   Time last 500 iters: 7.06s\n",
      "[Epoch 1, Iter 24500] Train MSE so far=0.113954 (ctrl+delta)   Time last 500 iters: 7.06s\n",
      "[Epoch 1, Iter 25000] Train MSE so far=0.113037 (ctrl+delta)   Time last 500 iters: 7.42s\n",
      "[Epoch 1, Iter 25500] Train MSE so far=0.113132 (ctrl+delta)   Time last 500 iters: 7.05s\n",
      "[Epoch 1, Iter 26000] Train MSE so far=0.113365 (ctrl+delta)   Time last 500 iters: 7.05s\n",
      "[Epoch 1, Iter 26500] Train MSE so far=0.114079 (ctrl+delta)   Time last 500 iters: 7.06s\n",
      "[Epoch 1, Iter 27000] Train MSE so far=0.113666 (ctrl+delta)   Time last 500 iters: 7.05s\n",
      "[Epoch 1, Iter 27500] Train MSE so far=0.114073 (ctrl+delta)   Time last 500 iters: 7.05s\n",
      "[Epoch 1, Iter 28000] Train MSE so far=0.113853 (ctrl+delta)   Time last 500 iters: 7.05s\n",
      "[Epoch 1, Iter 28500] Train MSE so far=0.113411 (ctrl+delta)   Time last 500 iters: 7.05s\n",
      "[Epoch 1, Iter 29000] Train MSE so far=0.113004 (ctrl+delta)   Time last 500 iters: 7.03s\n",
      "[Epoch 1, Iter 29500] Train MSE so far=0.113197 (ctrl+delta)   Time last 500 iters: 7.05s\n",
      "[Epoch 1, Iter 30000] Train MSE so far=0.112854 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 30500] Train MSE so far=0.113821 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 31000] Train MSE so far=0.113315 (ctrl+delta)   Time last 500 iters: 7.03s\n",
      "[Epoch 1, Iter 31500] Train MSE so far=0.113397 (ctrl+delta)   Time last 500 iters: 7.03s\n",
      "[Epoch 1, Iter 32000] Train MSE so far=0.112929 (ctrl+delta)   Time last 500 iters: 7.05s\n",
      "[Epoch 1, Iter 32500] Train MSE so far=0.113505 (ctrl+delta)   Time last 500 iters: 7.03s\n",
      "[Epoch 1, Iter 33000] Train MSE so far=0.113705 (ctrl+delta)   Time last 500 iters: 7.02s\n",
      "[Epoch 1, Iter 33500] Train MSE so far=0.113652 (ctrl+delta)   Time last 500 iters: 7.02s\n",
      "[Epoch 1, Iter 34000] Train MSE so far=0.113866 (ctrl+delta)   Time last 500 iters: 7.02s\n",
      "[Epoch 1, Iter 34500] Train MSE so far=0.113478 (ctrl+delta)   Time last 500 iters: 7.03s\n",
      "[Epoch 1, Iter 35000] Train MSE so far=0.113370 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 35500] Train MSE so far=0.113773 (ctrl+delta)   Time last 500 iters: 6.99s\n",
      "[Epoch 1, Iter 36000] Train MSE so far=0.113355 (ctrl+delta)   Time last 500 iters: 6.99s\n",
      "[Epoch 1, Iter 36500] Train MSE so far=0.113420 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 37000] Train MSE so far=0.113587 (ctrl+delta)   Time last 500 iters: 7.02s\n",
      "[Epoch 1, Iter 37500] Train MSE so far=0.113411 (ctrl+delta)   Time last 500 iters: 7.03s\n",
      "[Epoch 1, Iter 38000] Train MSE so far=0.113342 (ctrl+delta)   Time last 500 iters: 7.03s\n",
      "[Epoch 1, Iter 38500] Train MSE so far=0.113037 (ctrl+delta)   Time last 500 iters: 7.02s\n",
      "[Epoch 1, Iter 39000] Train MSE so far=0.113680 (ctrl+delta)   Time last 500 iters: 7.01s\n",
      "[Epoch 1, Iter 39500] Train MSE so far=0.113243 (ctrl+delta)   Time last 500 iters: 7.02s\n",
      "[Epoch 1, Iter 40000] Train MSE so far=0.113243 (ctrl+delta)   Time last 500 iters: 7.02s\n",
      "[Epoch 1, Iter 40500] Train MSE so far=0.113423 (ctrl+delta)   Time last 500 iters: 7.03s\n",
      "[Epoch 1, Iter 41000] Train MSE so far=0.113148 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 41500] Train MSE so far=0.113242 (ctrl+delta)   Time last 500 iters: 7.03s\n",
      "[Epoch 1, Iter 42000] Train MSE so far=0.113322 (ctrl+delta)   Time last 500 iters: 7.03s\n",
      "[Epoch 1, Iter 42500] Train MSE so far=0.113247 (ctrl+delta)   Time last 500 iters: 7.00s\n",
      "[Epoch 1, Iter 43000] Train MSE so far=0.113570 (ctrl+delta)   Time last 500 iters: 6.95s\n",
      "[Epoch 1, Iter 43500] Train MSE so far=0.113361 (ctrl+delta)   Time last 500 iters: 7.03s\n",
      "[Epoch 1, Iter 44000] Train MSE so far=0.113527 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 44500] Train MSE so far=0.113836 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 45000] Train MSE so far=0.113403 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 45500] Train MSE so far=0.113039 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 46000] Train MSE so far=0.113403 (ctrl+delta)   Time last 500 iters: 7.03s\n",
      "[Epoch 1, Iter 46500] Train MSE so far=0.113398 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 47000] Train MSE so far=0.112844 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 47500] Train MSE so far=0.113318 (ctrl+delta)   Time last 500 iters: 7.03s\n",
      "[Epoch 1, Iter 48000] Train MSE so far=0.113181 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 48500] Train MSE so far=0.113491 (ctrl+delta)   Time last 500 iters: 7.03s\n",
      "[Epoch 1, Iter 49000] Train MSE so far=0.113265 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 49500] Train MSE so far=0.112983 (ctrl+delta)   Time last 500 iters: 7.03s\n",
      "[Epoch 1, Iter 50000] Train MSE so far=0.113069 (ctrl+delta)   Time last 500 iters: 7.02s\n",
      "[Epoch 1, Iter 50500] Train MSE so far=0.113564 (ctrl+delta)   Time last 500 iters: 6.97s\n",
      "[Epoch 1, Iter 51000] Train MSE so far=0.113256 (ctrl+delta)   Time last 500 iters: 6.99s\n",
      "[Epoch 1, Iter 51500] Train MSE so far=0.113750 (ctrl+delta)   Time last 500 iters: 6.97s\n",
      "[Epoch 1, Iter 52000] Train MSE so far=0.113476 (ctrl+delta)   Time last 500 iters: 6.99s\n",
      "[Epoch 1, Iter 52500] Train MSE so far=0.113263 (ctrl+delta)   Time last 500 iters: 6.95s\n",
      "[Epoch 1, Iter 53000] Train MSE so far=0.113360 (ctrl+delta)   Time last 500 iters: 7.02s\n",
      "[Epoch 1, Iter 53500] Train MSE so far=0.113266 (ctrl+delta)   Time last 500 iters: 7.05s\n",
      "[Epoch 1, Iter 54000] Train MSE so far=0.113804 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 54500] Train MSE so far=0.113056 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 55000] Train MSE so far=0.113111 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 55500] Train MSE so far=0.113440 (ctrl+delta)   Time last 500 iters: 7.03s\n",
      "[Epoch 1, Iter 56000] Train MSE so far=0.113059 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 56500] Train MSE so far=0.113312 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 57000] Train MSE so far=0.113304 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 57500] Train MSE so far=0.113543 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 58000] Train MSE so far=0.113501 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 58500] Train MSE so far=0.113018 (ctrl+delta)   Time last 500 iters: 7.04s\n",
      "[Epoch 1, Iter 59000] Train MSE so far=0.113341 (ctrl+delta)   Time last 500 iters: 7.03s\n"
     ]
    }
   ],
   "source": [
    "model.train(dl, lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 20:13:32,388 - INFO - Running evaluation\n",
      "2025-02-19 20:13:32,389 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/DatlingerBock2017.yaml\n",
      "2025-02-19 20:13:32,391 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/SchraivogelSteinmetz2020_TAP_SCREEN__chromosome_11_screen.yaml\n",
      "2025-02-19 20:13:32,393 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/ReplogleWeissman2022_K562_essential.yaml\n",
      "2025-02-19 20:13:32,394 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/GasperiniShendure2019_atscale.yaml\n",
      "2025-02-19 20:13:32,395 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/TianKampmann2021_CRISPRa.yaml\n",
      "2025-02-19 20:13:32,397 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/repogle_k562_essential_raw.yaml\n",
      "2025-02-19 20:13:32,398 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/FrangiehIzar2021_RNA.yaml\n",
      "2025-02-19 20:13:32,400 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/AdamsonWeissman2016_GSM2406681_10X010.yaml\n",
      "2025-02-19 20:13:32,401 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/PapalexiSatija2021_eccite_RNA.yaml\n",
      "2025-02-19 20:13:32,402 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/TianKampmann2021_CRISPRi.yaml\n",
      "2025-02-19 20:13:32,404 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/ChangYe2021.yaml\n",
      "2025-02-19 20:13:32,405 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/TianKampmann2019_iPSC.yaml\n",
      "2025-02-19 20:13:32,406 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/PapalexiSatija2021_eccite_arrayed_RNA.yaml\n",
      "2025-02-19 20:13:32,408 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/AdamsonWeissman2016_GSM2406677_10X005.yaml\n",
      "2025-02-19 20:13:32,409 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/SrivatsanTrapnell2020_sciplex4.yaml\n",
      "2025-02-19 20:13:32,410 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/GehringPachter2019.yaml\n",
      "2025-02-19 20:13:32,411 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/ShifrutMarson2018.yaml\n",
      "2025-02-19 20:13:32,413 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/WeinrebKlein2020.yaml\n",
      "2025-02-19 20:13:32,414 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/satija_IFNB_raw.yaml\n",
      "2025-02-19 20:13:32,416 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/ReplogleWeissman2022_rpe1.yaml\n",
      "2025-02-19 20:13:32,417 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/AdamsonWeissman2016_GSM2406675_10X001.yaml\n",
      "2025-02-19 20:13:32,418 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/DixitRegev2016.yaml\n",
      "2025-02-19 20:13:32,420 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/SrivatsanTrapnell2020_sciplex2.yaml\n",
      "2025-02-19 20:13:32,421 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/NormanWeissman2019_filtered.yaml\n",
      "2025-02-19 20:13:32,422 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/FrangiehIzar2021_protein.yaml\n",
      "2025-02-19 20:13:32,423 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/satija_IFNB_HVG.yaml\n",
      "2025-02-19 20:13:32,425 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/kang.yaml\n",
      "2025-02-19 20:13:32,426 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/DatlingerBock2021.yaml\n",
      "2025-02-19 20:13:32,427 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/ReplogleWeissman2022_K562_gwps.yaml\n",
      "2025-02-19 20:13:32,429 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/essential_gene_knockouts_raw.yaml\n",
      "2025-02-19 20:13:32,430 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/GasperiniShendure2019_highMOI.yaml\n",
      "2025-02-19 20:13:32,432 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/GasperiniShendure2019_lowMOI.yaml\n",
      "2025-02-19 20:13:32,433 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/ZhaoSims2021.yaml\n",
      "2025-02-19 20:13:32,434 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/TianKampmann2019_day7neuron.yaml\n",
      "2025-02-19 20:13:32,436 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/PapalexiSatija2021_eccite_protein.yaml\n",
      "2025-02-19 20:13:32,437 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/SrivatsanTrapnell2020_sciplex3.yaml\n",
      "2025-02-19 20:13:32,438 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/XieHon2017.yaml\n",
      "2025-02-19 20:13:32,439 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/PapalexiSatija2021_eccite_arrayed_protein.yaml\n",
      "2025-02-19 20:13:32,440 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/SchraivogelSteinmetz2020_TAP_SCREEN__chromosome_8_screen.yaml\n",
      "2025-02-19 20:13:32,442 - INFO - Loading data catalogue from /orcd/data/omarabu/001/njwfish/omnicell/configs/catalogue/McFarlandTsherniak2020.yaml\n",
      "2025-02-19 20:13:36,988 - WARNING - No data found for cell: k562, pert: U2AF1 in repogle_k562_essential_raw, will skip evaluation\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "logger.info(\"Running evaluation\")\n",
    "\n",
    "# evaluate each pair of cells and perts\n",
    "eval_dict = {}\n",
    "for cell_id, pert_id, ctrl_data, gt_data in loader.get_eval_data():\n",
    "    logger.debug(f\"Making predictions for cell: {cell_id}, pert: {pert_id}\")\n",
    "\n",
    "    preds = model.make_predict(ctrl_data, pert_id, cell_id)\n",
    "    eval_dict[(cell_id, pert_id)] = (ctrl_data.X.toarray(), gt_data.X.toarray(), preds)\n",
    "    \n",
    "if not config.etl_config.log1p:\n",
    "    for (cell, pert) in eval_dict:  \n",
    "        ctrl_data, gt_data, pred_pert = eval_dict[(cell, pert)]\n",
    "        # normalize to sum to 1\n",
    "        ctrl_data = ctrl_data / ctrl_data.sum(axis=1).reshape(-1, 1) * 10_000\n",
    "        gt_data = gt_data / gt_data.sum(axis=1).reshape(-1, 1) * 10_000\n",
    "        pred_pert = pred_pert / pred_pert.sum(axis=1).reshape(-1, 1) * 10_000\n",
    "        eval_dict[(cell, pert)] =  (np.log1p(ctrl_data), np.log1p(gt_data), np.log1p(pred_pert))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/njwfish/miniconda3/envs/omnicell/lib/python3.9/site-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/home/njwfish/miniconda3/envs/omnicell/lib/python3.9/site-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "/home/njwfish/miniconda3/envs/omnicell/lib/python3.9/site-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/home/njwfish/miniconda3/envs/omnicell/lib/python3.9/site-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "from omnicell.evaluation.utils import get_DEGs, get_eval, get_DEG_Coverage_Recall, get_DEGs_overlaps\n",
    "pval_threshold = 0.05\n",
    "log_fold_change_threshold = 0.0\n",
    "\n",
    "results_dict = {}\n",
    "for (cell, pert) in eval_dict:  \n",
    "    ctrl_data, gt_data, pred_pert = eval_dict[(cell, pert)]\n",
    "\n",
    "    pred_pert = sc.AnnData(X=pred_pert)\n",
    "    true_pert = sc.AnnData(X=gt_data)\n",
    "    control = sc.AnnData(X=ctrl_data)\n",
    "\n",
    "    logger.debug(f\"Getting ground Truth DEGs for {pert} and {cell}\")\n",
    "    true_DEGs_df = get_DEGs(control, true_pert)\n",
    "    signif_true_DEG = true_DEGs_df[true_DEGs_df['pvals_adj'] < pval_threshold]\n",
    "\n",
    "    logger.debug(f\"Number of significant DEGS from ground truth: {signif_true_DEG.shape[0]}\")\n",
    "\n",
    "    logger.debug(f\"Getting predicted DEGs for {pert} and {cell}\")\n",
    "    pred_DEGs_df = get_DEGs(control, pred_pert)\n",
    "\n",
    "\n",
    "    logger.debug(f\"Getting evaluation metrics for {pert} and {cell}\")\n",
    "    r2_and_mse = get_eval(control, true_pert, pred_pert, true_DEGs_df, [100,50,20], pval_threshold, log_fold_change_threshold)\n",
    "\n",
    "    logger.debug(f\"Getting DEG overlaps for {pert} and {cell}\")\n",
    "    DEGs_overlaps = get_DEGs_overlaps(true_DEGs_df, pred_DEGs_df, [100,50,20], pval_threshold, log_fold_change_threshold)\n",
    "\n",
    "    results_dict[(cell, pert)] = (r2_and_mse, DEGs_overlaps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell: k562, Pert: RPL15\n",
      "DEGs Overlaps: {'Overlap_in_top_2157_DEGs': 645, 'Overlap_in_top_100_DEGs': 36, 'Overlap_in_top_50_DEGs': 11, 'Overlap_in_top_20_DEGs': 1, 'Jaccard': 0.15932572050027188}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cell: k562, Pert: RPL4\n",
      "DEGs Overlaps: {'Overlap_in_top_3714_DEGs': 1512, 'Overlap_in_top_100_DEGs': 31, 'Overlap_in_top_50_DEGs': 13, 'Overlap_in_top_20_DEGs': 6, 'Jaccard': 0.332315743567379}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cell: k562, Pert: RPL7\n",
      "DEGs Overlaps: {'Overlap_in_top_3845_DEGs': 1668, 'Overlap_in_top_100_DEGs': 34, 'Overlap_in_top_50_DEGs': 13, 'Overlap_in_top_20_DEGs': 5, 'Jaccard': 0.37248359470581693}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cell: k562, Pert: RNF113A\n",
      "DEGs Overlaps: {'Overlap_in_top_2852_DEGs': 194, 'Overlap_in_top_100_DEGs': 5, 'Overlap_in_top_50_DEGs': 0, 'Overlap_in_top_20_DEGs': 0, 'Jaccard': 0.033242258652094715}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cell: k562, Pert: TRRAP\n",
      "DEGs Overlaps: {'Overlap_in_top_3483_DEGs': 995, 'Overlap_in_top_100_DEGs': 48, 'Overlap_in_top_50_DEGs': 16, 'Overlap_in_top_20_DEGs': 3, 'Jaccard': 0.22505112474437627}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cell: k562, Pert: RPL13\n",
      "DEGs Overlaps: {'Overlap_in_top_3408_DEGs': 1402, 'Overlap_in_top_100_DEGs': 31, 'Overlap_in_top_50_DEGs': 15, 'Overlap_in_top_20_DEGs': 4, 'Jaccard': 0.3389163669064748}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cell: k562, Pert: RPS3\n",
      "DEGs Overlaps: {'Overlap_in_top_4320_DEGs': 2259, 'Overlap_in_top_100_DEGs': 59, 'Overlap_in_top_50_DEGs': 27, 'Overlap_in_top_20_DEGs': 3, 'Jaccard': 0.43593767514852594}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cell: k562, Pert: DDX18\n",
      "DEGs Overlaps: {'Overlap_in_top_2389_DEGs': 889, 'Overlap_in_top_100_DEGs': 58, 'Overlap_in_top_50_DEGs': 19, 'Overlap_in_top_20_DEGs': 4, 'Jaccard': 0.2239208633093525}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cell: k562, Pert: RPL27A\n",
      "DEGs Overlaps: {'Overlap_in_top_3724_DEGs': 1450, 'Overlap_in_top_100_DEGs': 29, 'Overlap_in_top_50_DEGs': 12, 'Overlap_in_top_20_DEGs': 5, 'Jaccard': 0.3290189253861214}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cell: k562, Pert: SNRPD3\n",
      "DEGs Overlaps: {'Overlap_in_top_3240_DEGs': 413, 'Overlap_in_top_100_DEGs': 6, 'Overlap_in_top_50_DEGs': 1, 'Overlap_in_top_20_DEGs': 0, 'Jaccard': 0.08608414239482201}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cell: k562, Pert: DNTTIP2\n",
      "DEGs Overlaps: {'Overlap_in_top_3430_DEGs': 1568, 'Overlap_in_top_100_DEGs': 64, 'Overlap_in_top_50_DEGs': 23, 'Overlap_in_top_20_DEGs': 2, 'Jaccard': 0.32916899051868376}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cell: k562, Pert: PHB\n",
      "DEGs Overlaps: {'Overlap_in_top_4535_DEGs': 789, 'Overlap_in_top_100_DEGs': 20, 'Overlap_in_top_50_DEGs': 8, 'Overlap_in_top_20_DEGs': 5, 'Jaccard': 0.10410842863193562}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cell: k562, Pert: WDR36\n",
      "DEGs Overlaps: {'Overlap_in_top_2671_DEGs': 1094, 'Overlap_in_top_100_DEGs': 63, 'Overlap_in_top_50_DEGs': 22, 'Overlap_in_top_20_DEGs': 2, 'Jaccard': 0.27304843304843307}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cell: k562, Pert: TSR2\n",
      "DEGs Overlaps: {'Overlap_in_top_5259_DEGs': 3027, 'Overlap_in_top_100_DEGs': 64, 'Overlap_in_top_50_DEGs': 26, 'Overlap_in_top_20_DEGs': 5, 'Jaccard': 0.49820398389027976}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cell: k562, Pert: RPL10\n",
      "DEGs Overlaps: {'Overlap_in_top_2812_DEGs': 989, 'Overlap_in_top_100_DEGs': 28, 'Overlap_in_top_50_DEGs': 12, 'Overlap_in_top_20_DEGs': 6, 'Jaccard': 0.27182829531408026}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cell: k562, Pert: XAB2\n",
      "DEGs Overlaps: {'Overlap_in_top_2669_DEGs': 180, 'Overlap_in_top_100_DEGs': 1, 'Overlap_in_top_50_DEGs': 0, 'Overlap_in_top_20_DEGs': 0, 'Jaccard': 0.029005524861878452}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cell: k562, Pert: EIF3A\n",
      "DEGs Overlaps: {'Overlap_in_top_3789_DEGs': 1389, 'Overlap_in_top_100_DEGs': 21, 'Overlap_in_top_50_DEGs': 11, 'Overlap_in_top_20_DEGs': 3, 'Jaccard': 0.2856544502617801}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cell: k562, Pert: LSM2\n",
      "DEGs Overlaps: {'Overlap_in_top_4053_DEGs': 736, 'Overlap_in_top_100_DEGs': 3, 'Overlap_in_top_50_DEGs': 0, 'Overlap_in_top_20_DEGs': 0, 'Jaccard': 0.11679287305122495}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Cell: k562, Pert: PCF11\n",
      "DEGs Overlaps: {'Overlap_in_top_3226_DEGs': 372, 'Overlap_in_top_100_DEGs': 11, 'Overlap_in_top_50_DEGs': 6, 'Overlap_in_top_20_DEGs': 0, 'Jaccard': 0.0524048815506102}\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for (cell, pert) in results_dict:\n",
    "    r2_and_mse, DEGs_overlaps = results_dict[(cell, pert)]\n",
    "    print(f\"Cell: {cell}, Pert: {pert}\")\n",
    "    # print(f\"R2 and MSE: {r2_and_mse}\")\n",
    "    print(f\"DEGs Overlaps: {DEGs_overlaps}\")\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 {'all_genes_mean_sub_diff_R': np.float32(-0.12489348), 'all_genes_mean_sub_diff_R2': np.float32(0.015598381), 'all_genes_mean_sub_diff_MSE': np.float32(0.068853445), 'all_genes_mean_fold_diff_R': np.float32(-0.46685344), 'all_genes_mean_fold_diff_R2': np.float32(0.21795213), 'all_genes_mean_fold_diff_MSE': np.float32(24.076696), 'all_genes_mean_R': np.float32(0.88978803), 'all_genes_mean_R2': np.float32(0.7917227), 'all_genes_mean_MSE': np.float32(0.068853445), 'all_genes_var_R': np.float32(0.48825538), 'all_genes_var_R2': np.float32(0.23839332), 'all_genes_var_MSE': np.float32(0.02451049), 'all_genes_corr_mtx_R': np.float64(0.2062134434958776), 'all_genes_corr_mtx_R2': np.float64(0.04252398427842751), 'all_genes_corr_mtx_MSE': np.float64(0.0030738678439887306), 'all_genes_cov_mtx_R': np.float64(0.2403351520107548), 'all_genes_cov_mtx_R2': np.float64(0.05776098529203262), 'all_genes_cov_mtx_MSE': np.float64(9.166059826543978e-05), 'Top_3226_DEGs_sub_diff_R': np.float32(-0.13143796), 'Top_3226_DEGs_sub_diff_R2': np.float32(0.017275937), 'Top_3226_DEGs_sub_diff_MSE': np.float32(0.10873328), 'Top_3226_DEGs_fold_diff_R': np.float32(-0.5040652), 'Top_3226_DEGs_fold_diff_R2': np.float32(0.25408176), 'Top_3226_DEGs_fold_diff_MSE': np.float32(56.589275), 'Top_3226_DEGs_mean_R': np.float32(0.8892034), 'Top_3226_DEGs_mean_R2': np.float32(0.79068273), 'Top_3226_DEGs_mean_MSE': np.float32(0.10873328), 'Top_3226_DEGs_var_R': np.float32(0.316803), 'Top_3226_DEGs_var_R2': np.float32(0.10036415), 'Top_3226_DEGs_var_MSE': np.float32(0.030848827), 'Top_3226_DEGs_corr_mtx_R': np.float64(0.2614002138049976), 'Top_3226_DEGs_corr_mtx_R2': np.float64(0.06833007177729847), 'Top_3226_DEGs_corr_mtx_MSE': np.float64(0.005361015224219908), 'Top_3226_DEGs_cov_mtx_R': np.float64(0.2621316276344985), 'Top_3226_DEGs_cov_mtx_R2': np.float64(0.06871299020631137), 'Top_3226_DEGs_cov_mtx_MSE': np.float64(0.0002308022100430066), 'Top_100_DEGs_sub_diff_R': np.float32(-0.34805655), 'Top_100_DEGs_sub_diff_R2': np.float32(0.12114336), 'Top_100_DEGs_sub_diff_MSE': np.float32(0.518265), 'Top_100_DEGs_fold_diff_R': np.float32(-0.6485699), 'Top_100_DEGs_fold_diff_R2': np.float32(0.42064288), 'Top_100_DEGs_fold_diff_MSE': np.float32(1138.2806), 'Top_100_DEGs_mean_R': np.float32(0.9234525), 'Top_100_DEGs_mean_R2': np.float32(0.8527645), 'Top_100_DEGs_mean_MSE': np.float32(0.518265), 'Top_100_DEGs_var_R': np.float32(-0.23653984), 'Top_100_DEGs_var_R2': np.float32(0.055951096), 'Top_100_DEGs_var_MSE': np.float32(0.045268893), 'Top_100_DEGs_corr_mtx_R': np.float64(0.24930696164913915), 'Top_100_DEGs_corr_mtx_R2': np.float64(0.06215396112672534), 'Top_100_DEGs_corr_mtx_MSE': np.float64(0.08367932638355825), 'Top_100_DEGs_cov_mtx_R': np.float64(0.2790809789402299), 'Top_100_DEGs_cov_mtx_R2': np.float64(0.07788619280623703), 'Top_100_DEGs_cov_mtx_MSE': np.float64(0.006884069580873242), 'Top_50_DEGs_sub_diff_R': np.float32(-0.43575236), 'Top_50_DEGs_sub_diff_R2': np.float32(0.18988012), 'Top_50_DEGs_sub_diff_MSE': np.float32(0.6234409), 'Top_50_DEGs_fold_diff_R': np.float32(-0.6672086), 'Top_50_DEGs_fold_diff_R2': np.float32(0.44516733), 'Top_50_DEGs_fold_diff_MSE': np.float32(1068.1506), 'Top_50_DEGs_mean_R': np.float32(0.95721245), 'Top_50_DEGs_mean_R2': np.float32(0.91625565), 'Top_50_DEGs_mean_MSE': np.float32(0.6234409), 'Top_50_DEGs_var_R': np.float32(-0.18109739), 'Top_50_DEGs_var_R2': np.float32(0.032796264), 'Top_50_DEGs_var_MSE': np.float32(0.046017352), 'Top_50_DEGs_corr_mtx_R': np.float64(0.2943858858365494), 'Top_50_DEGs_corr_mtx_R2': np.float64(0.0866630497797699), 'Top_50_DEGs_corr_mtx_MSE': np.float64(0.10856303706476064), 'Top_50_DEGs_cov_mtx_R': np.float64(0.3147749233852356), 'Top_50_DEGs_cov_mtx_R2': np.float64(0.09908325239218095), 'Top_50_DEGs_cov_mtx_MSE': np.float64(0.010610934213028259), 'Top_20_DEGs_sub_diff_R': np.float32(-0.4751064), 'Top_20_DEGs_sub_diff_R2': np.float32(0.22572608), 'Top_20_DEGs_sub_diff_MSE': np.float32(0.9028176), 'Top_20_DEGs_fold_diff_R': np.float32(-0.7971266), 'Top_20_DEGs_fold_diff_R2': np.float32(0.6354108), 'Top_20_DEGs_fold_diff_MSE': np.float32(2549.8733), 'Top_20_DEGs_mean_R': np.float32(0.96330136), 'Top_20_DEGs_mean_R2': np.float32(0.9279495), 'Top_20_DEGs_mean_MSE': np.float32(0.9028176), 'Top_20_DEGs_var_R': np.float32(-0.04642755), 'Top_20_DEGs_var_R2': np.float32(0.0021555175), 'Top_20_DEGs_var_MSE': np.float32(0.061873764), 'Top_20_DEGs_corr_mtx_R': np.float64(0.41181288907182567), 'Top_20_DEGs_corr_mtx_R2': np.float64(0.1695898556056838), 'Top_20_DEGs_corr_mtx_MSE': np.float64(0.12525902388144505), 'Top_20_DEGs_cov_mtx_R': np.float64(0.3689636662611385), 'Top_20_DEGs_cov_mtx_R2': np.float64(0.1361341870208608), 'Top_20_DEGs_cov_mtx_MSE': np.float64(0.01642593539246946)}\n"
     ]
    }
   ],
   "source": [
    "r2_and_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
